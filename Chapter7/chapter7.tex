
%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Third Chapter **********************************
%*******************************************************************************
\chapter{Reconstructing SBND}
\label{ChapterReco}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter7/Figs/Raster/}{Chapter7/Figs/PDF/}{Chapter7/Figs/}}
\else
    \graphicspath{{Chapter7/Figs/Vector/}{Chapter7/Figs/}}
\fi

%********************************** %Opening  **************************************

Reconstruction is the process of extracting physics quantities from the raw detector data.
For SBND, the goal of reconstruction is to reconstruct particles that deposit energy within the detection subsystems.
A contained particle inside the TPC might deposit energy resulting in only waveforms recorded by the TPC wire planes and the PDS, meanwhile, an exiting particle might deposit additional energy in CRT walls surrounding the TPC.
For a given particle, the TPC reconstruction can extract various quantities describing its topology, calorimetry and kinematics.
The PDS reconstruction can provide additional high resolution timing information of the particle on the top of its light calorimetry.
The CRT reconstruction can indicate if the particle is fully contained inside the detector.
As a result, the reconstruction variables from all three detection subsystems are complementary to each other and can be exploited for different analysis purposes such as cosmic rejection or particle identification.

This chapter provides a summary of the reconstruction framework at SBND.
The first Sec. \ref{sec:reco_overview} gives an overview of the reconstruction workflow for each detection subsystem. 
The following Sec. \ref{sec:reco_tpc} provides the details of the TPC reconstruction workflow from start to end.
Meanwhile, the reconstruction of the PDS and CRT subsystem are summarised in Sec. \ref{sec:reco_others}.
Furthermore, descriptions of some high-level analysis tools using the reconstruction variables collectively from each detection subsystem are included in Sec. \ref{sec:reco_ana_tools}.                 
Finally, the chapter is concluded in Sec. \ref{sec:reco_concluding_remarks} with some remarks.

\newpage

%********************************** %First Section  **************************************

\section{Overview of Reconstructing SBND}
\label{sec:reco_overview}

%Describe the overall workflow
Each detection subsystem in SBND requires a dedicated reconstruction workflow.
An overview of each workflow is illustrated in Fig. \ref{fig:Reco_Workflow}.
The TPC reconstruction workflow is shown by the red boxes.
This process begins with raw wire waveforms going through the signal processing performed by the Wirecell tool kit \cite{wirecell}.
This is followed by a hit finding algorithm to identify hits on the waveform.
Output hits are then used by the Pandora package \cite{pandora} to produce a 3D-reconstructed interaction, denoted as a \textit{slice}.
The PDS reconstruction workflow also follows a similar process to the TPC, as shown by the blue boxes.
A waveform deconvolution is first performed on raw PDS waveforms to filter noise.
Then, a hit finding algorithm identifies optical hits on the waveform and reconstruction is performed on the hits.
The equivalent output to the TPC-reconstructed interaction from the PDS reconstruction is referred to as a \textit{flash}.
Finally, the reconstruction for the CRTs is much simpler compared to the other two subsystems, consisting of only a hit finding and a reconstruction algorithm, as shown by the orange boxes.
The reconstruction variables from each detection subsystem are produced independently and can be matched together if they originate from the same interaction. 
The variables can also be combined and input into different high-level analysis tools to extract more
 complex properties of the underlying interaction. 

\begin{figure}[htbp!] 
\centering    
\includegraphics[width=1.0\textwidth]{Reco_Workflow}
\caption[Reco_Workflow]{
Overview of the reconstruction workflow of the SBND detector.
}
\label{fig:Reco_Workflow}
\end{figure}

\newpage
\section{TPC Reconstruction}
\label{sec:reco_tpc}

\subsection{TPC Signal Processing}
%Signal Processing
Signal processing is the first crucial step of TPC reconstruction, which is to deconvolve digitised raw waveforms and remove detector effects such as noise, electronics response and field response. 
At SBND, signal processing is implemented using the WireCell tool kit \cite{wirecell}.
The tool has been used by other various LArTPC experiments such as MicroBooNE and ProtoDUNE and has demonstrated excellent performance to acquire deconvolved charge by performing deconvolution in time and wire dimension over the traditional deconvolution in time dimension only. 

Fig. \ref{fig:signal_processing_steps} \cite{LynnSignal} demonstrates the step by step of the signal processing.
In grey is the true charge deposition on a wire, and in red is the corresponding raw waveform containing the signal convolved with noise, electronics response, field response and noise.
The first step in the chain is noise filtering to remove the excess and correlated noise from raw waveforms.
Then, the measured charge is deconvolved from the electronics and field response to recover the original charge deposited on the wire, as shown in orange.
The deconvolution is 2D, where response functions consider the time response of a single wire as well as the responses of neighbouring wires.
This step is particularly important for the induction planes to convert bipolar into unipolar signals, such that the integral of the waveform can be used for charge estimation.
\begin{figure}[tbp!] 
\centering    
\includegraphics[width=0.45\textwidth]{signal_processing_steps}
\caption[signal_processing_steps]{
Example demonstrating the steps of signal processing applied to a bipolar raw waveform.
Fig. from Ref. \cite{LynnSignal}.
}
\label{fig:signal_processing_steps}
%\end{figure}
%\begin{figure}[htbp!]
\vspace{1cm}
\centering    
\includegraphics[width=0.75\textwidth]{signal_processing_waveform}
\caption[signal_processing_waveform]{
Event display of a simulated neutrino event using true ionisation charges (left), raw waveforms (middle) and deconvolved waveforms (right).
Fig. from Ref. \cite{LynnSignal}.
}
\label{fig:signal_processing_waveform}
\end{figure}
Filter functions are applied subsequently to attenuate noise that is artificially amplified.
This includes high frequency filters to remove high frequency noise, whereas Gaussian or Weiner filters can be used depending on whether the signal is unipolar or bipolar.
The example shown here is a bipolar signal waveform and therefore has both Gaussian and Weiner filters applied.
Then, low frequency filters are utilised for region-of-interest finding and local baseline removal, as shown in blue.
Finally, the deconvolved waveform per wire after baseline removal is shown in purple.

Fig. \ref{fig:signal_processing_waveform} depicts event displays of a simulated neutrino event as recorded by wires on the u plane.
The left figure displays the event using true charge depositions on wires.
The middle figure shows the event using raw waveforms, whilst the right figure shows the same event using deconvolved waveforms.
Recently, the deconvolution and filtering algorithms have been optimised specifically for the SBND detector.
This is demonstrated in the right event display showing clean signals.
Two tracks and two showers can be observed, closely resemblance the true charge deposition. 

\subsection{TPC Hit Finding}

After signal processing, hit finding is performed on deconvolved waveforms to search for Gaussian-shaped pulses above a threshold.
This is done by the \texttt{GausHitFinder} module \cite{gaushitfinder} by fitting a series of Gaussians to the waveform.                                                                                  
The number of pulses is determined by the number of maxima found when differentiating the waveform, where each pulse represents a hit.                                                                   
Fig. \ref{fig:gaushit} \cite{EdPhD} demonstrates the hit finding process for a deconvolved waveform, showing four hits have been identified and fitted with a Gaussian.
Once the hits are fitted, information describing the hit is extracted and used by downstream pattern recognition and reconstruction.
The peak time represents the time at which the charge arrives at the wire, used for determining the drift position and matching hit coincidence across wire planes.
The height and the width of the Gaussian are used to calculate the integral of the pulse, representing the deposited charge on the wire, subsequently used in downstream analysis for calorimetry computation.

\begin{figure}[htbp!] 
\centering    
\includegraphics[width=0.5\textwidth]{gaushit}
\caption[gaushit]{
Diagram illustrating the hit finding algorithm performance on a single wire.
Fig. from Ref. \cite{EdPhD}.
}
\label{fig:gaushit}
\end{figure}

\subsection{Pandora Pattern Recognition}

%Pattern Recognition: Pandora
The output hits from the hit finding process are then used to perform 3D reconstruction, which is performed with the Pandora pattern recognition package\cite{pandora}.
The package was first developed for the International Linear Collider and later extended to other LArTPC experiments.
It is made up of over 100 individual algorithms, performing a specific task along the reconstruction chain.
The output from Pandora represents an interaction, containing a hierarchy of particles starting with a neutrino parent at the interaction vertex.
This reconstructed object is referred to as a \textit{slice}.

The reconstruction begins with a workflow to reconstruct cosmic-like objects that leave long tracks inside the detector.
This workflow performs a 2D clustering on each wire plane independently, followed by 3D reconstruction under the assumption that all clusters are track-like.
Then, a cosmic rejection is performed to identify if a cluster is cosmic-like or neutrino-like.
The cosmic removal at this stage is deliberately cautious such that only very unambiguous cosmic muons in nature are removed.

The remaining clusters are then input into a second workflow dedicated towards neutrino reconstruction.
This workflow begins with a slicing algorithm that divides clusters into \textit{slices}, where each slice encapsulates hits coming from a single origin, representing an interaction.
Then, 2D clustering is re-performed on each wire plane independently, however, with a new assumption that clusters can be both track-like and shower-like.
A vertexing algorithm then identifies the interaction vertex of the slice and its associated clusters.
A series of pattern matching algorithms grows the interaction out of the neutrino vertex and performs 3D reconstruction by matching 2D clusters across different planes.
The output 3D reconstructed object associated with a vertex ideally represents a \textit{particle} produced from an interaction.

At this stage, a \textit{track score} is assigned to a particle if it has a track-like or a shower-like topology, which is determined by a dedicated Boosted Decision Tree (BDT).
The development work on the track-shower separation BDT and its importance not only in the reconstruction workflow but also in the analysis workflow will be covered in the next Sec. \ref{sec:trkshwbdt}.
Both track and shower reconstruction tools are then performed on the particle. 
Finally, a hierarchy algorithm is performed to classify the hierarchy of particles in a slice, starting with a neutrino parent vertex, and other particles are children, grandchildren, etc. of the parent.                             

%Calorimetry reconstruction
The last stage of reconstruction is calorimetry computation for the output slices and their associated particles.
Both track and shower calorimetry computations first convert ADC units to charges, or number of electrons, via multiplying by a charge calibration constant.
The track calorimetry then computes the energy from the charge using the ModBox recombination formalism, factoring in the electric field distortion.
Meanwhile, the shower calorimetry reconstruction converts the measured charge to energy by multiplying it by a shower calibration constant, factoring in an averaged recombination factor. 
Once the SBND detector is operational, the calibration constants will be measured via dedicated calibration physics runs.
The charge calibration constant is expected to be computed by using a sample of the anode to cathode crossing muon tracks while the shower calibration constant can be acquired from using a standard candle of the neutral pion invariant mass \cite{uboone_gamma}.

\subsection{Track-Shower Separation Boosted Decision Tree}
\label{sec:trkshwbdt}
As previously discussed, reconstructed particles from Pandora are assigned with a track score determined by Boosted Decision Tree (BDT), which is a binary classification machine learning tool.
The track score spans between 0 and 1 such that if a particle has a very high track score close to 1, then the particle is track-like.
Otherwise, if the track score is very close to 0, then the particle is shower-like.

The track-shower BDT has become more important in the reconstruction as well as the analysis workflow due to a new reconstruction paradigm introduced by Pandora.
The traditional reconstruction approach was to perform only either track or shower reconstruction on a particle based on its track score.
Meanwhile, the new paradigm performs both track and shower reconstruction on a particle regardless of the track score.
All reconstructed particles now have two sets of reconstruction variables for track-like and shower-like.
The users have the freedom to decide which variables to use depending on their signal topology, and thus not pre-determined by Pandora.
The track score can inform on which appropriate reconstruction variables should be used for the analysis. 

The track-shower separation BDT was trained on a series of reconstruction variables and was updated to include new variables in the training.
The original BDT includes variables describing the geometrical topology of the particle such as its length, distance and direction with respect to the parent vertex, as well as calorimetry variables describing the charge distribution of the particle.
More details of the input variables and the training of the BDT can be found in Ref. \cite{EdPhD}.
The update extended beyond the original work to include a brand new set of variables describing how cone-like the charge distribution of a particle as well as a new variable describing the particle hierarchy.

The cone variables were first developed by the M. Haigh for particle identification \cite{warwick_pid}, and now imported into Pandora for reconstruction purposes.
There are three variables: (1) halo-total ratio, (2) concentration and (3) conicalness as depicted in Fig. \ref{fig:cone_variables}.
The diagrams depict the hit distribution of a hypothetical particle, where each circle represents a hit associated with a charge value and the star represents the vertex of the hit cluster.
The illustration is in 2D however the variables are computed in 3D.
The first variable is the halo-total ratio, illustrated in Fig. \ref{fig:halototalratio}.
The region outside of the Moliere radius, defined such that 90\% of the cluster energy is contained within this radius, is considered the halo region.
\begin{figure}[bp!]
        \centering
        \begin{subfigure}[b]{0.495\textwidth}
            \centering
            \includegraphics[width=\textwidth]{HaloTotalRatio}
            \caption{Halo Total Ratio}%
            \label{fig:halototalratio}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.495\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{Concentration}
            \caption{Concentration}%
            \label{fig:concentration}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.495\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{Conicalness}
            \caption{Conicalness}%
            \label{fig:conicalness}
        \end{subfigure}
        \caption[cone_variables]{
	Diagrams illustrating the variables describing how cone-like the charge distribution of a particle .
	}
        \label{fig:cone_variables}
\end{figure}
The hits in the halo are shown as green circles whereas any other hits are shown as grey circles.
The halo-total ratio is then defined as 
\begin{equation}
	Halo\ Total\ Ratio = \frac{Charges\ in\ The\ Halo}{Total\ Charges}
\end{equation}
The second variable is called concentration, accounting for how concentrated the charge distribution is to the centre of the cluster.
This is depicted in Fig. \ref{fig:concentration}, where each hit is assigned a colour showing how weighted it is with respect to its orthogonal distance to the cluster direction.
The closer the hit to the centre, the more weighted it is.
The concentration variable is defined as the total weighted charges divided by the total charge as following
\begin{equation}
	Concentration = \frac{\sum Charge \times Weight}{Total\ Charges}
\end{equation}
Finally, the conicalness variable examines the hit distribution at the end and the start of the cluster as depicted in Fig. \ref{fig:conicalness}. 
It is defined as the ratio between the concentration at the end of the cluster compared to at the start of the cluster
\begin{equation}
	Conincalness = \frac{Concentration\ at\ The\ End}{Concentration\ at\ The\ Start}
\end{equation}

On top of the cone variables, another new variable was introduced to the track-shower separation BDT to describe the hierarchy of the particle within the reconstructed interaction or slice.
For a given particle, the daughters originating from that particle are identified and their number of hits are counted.
The distributions of the four new variables for a track-like and shower-like particle are shown in Fig. \ref{fig:bdt_features}.
The concentration and conicalness variables display the strongest separation power between tracks and showers compared to the halo-total ratio and the number of daughter hits variables.

\begin{figure}[bp!]
        \centering
        \begin{subfigure}[b]{0.45\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Feature_Halo_Total_Ratio}
            \caption{}%
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.45\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{Feature_Concentration}
            \caption{}%
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.45\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{Feature_Conicalness}
            \caption{}%
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.45\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{Feature_Number_Of_Daughter_Hits}
            \caption{}%
        \end{subfigure}
        \caption[bdt_features]{
	Distributions of the new variables input into the track-shower separation BDT, plotted for track-like and shower-like particles.
	}
        \label{fig:bdt_features}
\end{figure}

Fig. \ref{fig:bdt_score} shows the score distribution of the BDT retrained with the four new variables.
The left figure shows two distinct distributions in red and blue for showers and tracks respectively.
This demonstrates a good separation power of the BDT, where particles with a score less than 0.5 closely resemble showers whilst particles with a score more than 0.5 are more track-like.
The score distribution is broken down into different particle types as shown in the right figure.
The distribution is expected given that electrons and photons leave electromagnetic shower activities inside the detector whilst charged particles like muons, pions and protons leave track-like signatures. 
The updated BDT resulted in $0.1\sim2.0\%$ improvement in correctly classifying particle type as shower-like or track-like.
The track-shower separation score distribution will be used in downstream high level analysis tools, to be detailed in Sec. \ref{sec:subsystem_match}, as well as will be employed as a cut variable in the HNL selection, to be detailed in Chapter \ref{ChapterSelect}.

\begin{figure}[htbp!]
        \centering
        \begin{subfigure}[b]{0.495\textwidth}
            \centering
            \includegraphics[width=\textwidth]{bdt_score_trk_shw}
            \caption{}%
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.495\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{bdt_score_particle}
            \caption{}%
        \end{subfigure}
        \caption[bdt_score]{
	Score distribution of the updated track-shower separation BDT, plotted for track-like and shower-like particles (left) and for particle type (right).
	}
        \label{fig:bdt_score}
\end{figure}

\section{Other Subsystems Reconstruction}
\label{sec:reco_others}

\subsection{PDS Reconstruction}
\label{sec:reco_pds}

The reconstructions for the photodetectors, PMTs and X-ARAPUCAs, share the same steps of waveform deconvolution, hit finding and light reconstruction.
However, different algorithms and parameter settings are required for each photodetector type due to their difference in response.
More details on the PDS reconstruction at the SBND detector can be found in Ref. \cite{sbnd_pds_paper}.
The following focuses on the reconstruction of PMTs which has an averaged Single Electron Response 
(SER) pulse peaking at $\sim 25$ ADC and a full width at half maximum of $\sim$ 10 ns.
The fast response of PMT plays a key role in the nanosecond timing resolution requirement for the HNL 
search.

Similarly to the signal processing in TPC reconstruction, the PMT waveform deconvolution also aims to remove noise and determine the waveform baseline.
Fig. \ref{fig:pds_reco_deconvolution} \cite{sbnd_pds_paper} depicts an example PMT waveform before and after the deconvolution.
The top figure shows the number of MC photons seen by a PMT as a function of time in green.
The middle figure shows the raw waveform in blue, convolved with the PMT response and noise.
The AC circuits of PMTs lead to over/undershoot features across the raw waveform with respect to the baseline.
The waveform deconvolution step consists of a 1D deconvolution and an application of a Gaussian filter to 
remove high frequency noise.                                                                            
The deconvolved waveform is shown in orange in the bottom figure.
The bipolarity features of the waveform are fully removed while the integral and temporal position of the peaks are still well-maintained.

\begin{figure}[tbp!]
        \centering
        \begin{subfigure}[b]{0.59\textwidth}
            \centering
            \includegraphics[width=\textwidth]{pds_reco_deconvolution}
            \caption{Waveform Deconvolution}
            \label{fig:pds_reco_deconvolution}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.4\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{pds_reco_hit_finding}
            \caption{Hit Finding}
            \label{fig:pds_reco_hit_finding}
        \end{subfigure}
        \caption[pds_reco]{
	Example demonstrating the waveform deconvolution and hit finding algorithprocessm applied to a raw PMT waveform. 
	Fig. from Ref. \cite{sbnd_pds_paper}.
	}
        \label{fig:pds_reco}
\end{figure}

The next step is to perform the hit finding on the deconvolved PMT waveform as demonstrated in Fig. \ref{fig:pds_reco_hit_finding}.
The hit finding begins with a baseline subtraction, which is calculated using a 400 ns portion at the start and end of the deconvolved waveform, resulting in the waveform shown in orange.
Optical hits are identified by finding pulses that go above a threshold of $1/4$ the amplitude of the deconvolved SER and 3 standard deviations away from the baseline root mean square.
The example shows 4 identified optical hits, with peak times denoted with red triangles.
The first optical hit contains multiple peaks merged into a single optical hit due to multiple photons arriving very closely in time to the PMT.
The rise time of an optical hit is then estimated as the time at which the pulse goes above 15\% of the largest peak amplitude, denoted with blue stars.
This results in a resolution of 1.6 ns in estimating the arrival time of the first photon contributing to the optical hit.
The integral of the optical hit is computed to estimate the number of PhotoElectrons (PEs) of the hit.

After the hit finding, the light reconstruction algorithm then clusters optical hits into an \textit{optical flash}.                                                                                            
The length of an optical flash is set as 8 $\mu$s to account for the total light produced in an interaction, from both prompt and slow components of scintillation photons.
The clustering algorithm is based on several conditions on the calorimetry of optical hits, timing distribution between hits and geometrical location of PMTs.                                                  
The number of PEs in an optical flash is the sum of PEs of optical hits clustered in that flash, ideally
 represent the total light generated by an interaction. 

The start time of the optical flash represents the start time of an interaction, $t_0$, which is the key
 variable of the HNL search, and therefore requires great care in reconstruction.
The flash start time is the average of the rise time of optical hits in a flash for the PMTs that contribute 50\% of the prompt light in the 30 ns window of the largest PE pulse.                              
Then, a correction is applied for the propagation of the photons from the scintillation position in the TPC to the PMTs, i.e. in the drift or $x$-direction.                                                    
This is done by exploiting the high density of PMTs as well as having PMTs sensitive to either or both 
direct and reflected light components.
As previously shown in Fig. \ref{fig:light_yield_Diego} with some discussions in Sec. \ref{sec:wls}, the
 arrival time distribution for the direct VUV and reflected visible photons varies with the drift distance.
For a given scintillation position, and therefore a drift position, the resulting amount of direct and visible photons leads to a specific ratio of the two components seen by PMTs.                     
Therefore, the ratio of the number of photons seen by TPB-coated PMTs to those seen by uncoated 
PMTs are used to compute the correction for the drift propagation effect.
The timing resolution of the flash time after the correction varies $\sim 2$ ns across the entire drift distance, demonstrating the excellent capability of the PDS reconstruction at SBND.

\subsection{CRT Reconstruction}

The reconstruction workflow for the CRT subsystem is the simplest compared to the TPC and the PDS.                                                                                                    
As previously detailed, the output data of the CRT readouts are in a group of 32 ADC values, for a single ADC per SiPM.
The reconstruction begins with a hit finding algorithm to identify which pair of SiPM in the group of 32 goes above a threshold.
The SiPM pair determines the lateral position of a cosmic muon hit within a CRT strip.
Then, a clustering algorithm groups hits from orthogonal CRT strips of the same wall within a 50 ns window to yield 3D space points.
For each CRT space point, the timing and calorimetry information are calculated and corrected for the propagation effect due to the longitudinal distance from the hit position to the SiPM.
The final step is to match space points from multiple CRT walls to form a CRT track.
Candidate tracks are evaluated from any combinations of space points from different walls within a 100 ns coincidence window.
The best track candidate is selected by the goodness of timing agreement and prioritising three-point tracks over two-point tracks. 
The outputs of the CRT reconstruction are both CRT space points and CRT tracks. 

%********************************** %First Section  **************************************
\section{High-Level Analysis Tools}
\label{sec:reco_ana_tools}

The reconstructed variables from each detection subsystem, slices from the TPC, flashes from the PDS, space points and tracks from the CRTs, can be used collectively by downstream algorithms to compute 
useful characteristics regarding the interaction.
The following section covers the main high-level analysis tools used in the HNL search, and the details of their usage will be discussed in Chapter \ref{ChapterSelect}.
Firstly, Sec. \ref{sec:subsystem_match} provides the details on how variables from different detection subsystems can be matched to the same interaction.
Furthermore, Sec. \ref{sec:crumbs} discusses the tools used for cosmic rejection, which is an abundant background due to SBND being an overground detector.                                                 
Finally, Sec. \ref{sec:razzled} illustrates the tools used for particle identification.

\subsection{Subsystems Matching Tools}
\label{sec:subsystem_match}

The reconstructed tracks using the TPC can be matched to a space point or tracks from the CRTs to provide additional information for cosmic rejection.
An example is that a through-going cosmic ray produces a long track in the TPC as well as deposit energy
 in the nearest CRT walls where the track starts and ends. 
Another example is that tracks produced in the TPC deposit energy in the nearest CRT strips only when they enter or exit the detector, enabling the tagging of stopping cosmic muons or exiting neutrinos.

Two types of matching are performed: (1) matching a TPC track to CRT space points and (2) matching a TPC track to CRT tracks.
The former method extrapolates the TPC track and matches with the nearest CRT space points by computing a distance of closest approach (DCA), confining the matching to a single CRT wall. 
The latter method uses a compound score from the average DCA stepping along the TPC track and the angle 
between the TPC and CRT tracks, enabling matching a single TPC track to two or more CRT walls.
Both method uses the timing information of the CRT objects for further constraints and no duplications in matching are allowed.

The second subsystem matching of interest is matching an interaction reconstructed using TPC wires, 
a slice, to an interaction reconstructed using PMTs, a flash.                                        
This matching is vitally important since the flash time matched to a slice represents the start time of the interaction reconstructed in that slice.
Therefore, it is the key variable separating the showers resulting from HNL decays from other SM observables.

The matching of the TPC to the PDS is done with the \texttt{Opt0Finder} module \cite{opt0finder_module}.
The matching is based on the estimation of charge yield in a slice seen by the wires to predict the light yield, and whether the prediction is in good agreement with the measured light yield seen by PMTs.
For a given slice, the charge yield is converted to energy to light yield based on the topology of particles in the slice.
The track score of the particles in the slice, assigned by the track-shower separation BDT, is used to indicate whether the particle is track-like or shower-like and therefore, which appropriate calorimetry computation to use.
If the particle is track-like, the calorimetry computation uses the ModBox recombination formalism with 
the charge-light anti-correlation. 
Otherwise, the calorimetry computation uses a charge-to-light conversion by multiplying a constant.     
From the estimated light yield, a hypothesis of the number of PE seen by each PMT is constructed by re-running the semi-analytical light library.
Finally, the hypothesis PE is compared to the measured PE for any given flash by a $\chi^2$ computation.
The flash that is best matched to a slice is the one with the lowest $\chi^2$.
Only one-to-one match is allowed such that only a single optical flash is matched to a slice.

A useful variable from this matching process is the comparison between the hypothesis PE predicted from the measured charges, denoted as $L_{\mathrm{Q}}$, and the measured PE seen by PMTs, denoted as $L$.
The variable is defined as follows
\begin{equation}
\label{eq:opt0fraction}
	\frac{L_{\mathrm{Q}} - L}{L}
\end{equation}
This fraction indicates the level of agreement between $L_{\mathrm{Q}}$ and $L$, and thus, the level of agreement in calorimetry measurement for the same interaction using the charge yield or the light yield. 
If the fraction is positive, the hypothesis PE from charge $L_{\mathrm{Q}}$ is over predicted, otherwise, it is underestimated.
This variable is particularly useful in the selection of HNL showers due to their forward-going topology, which will be detailed in Chapter \ref{ChapterSelect}.

\subsection{Cosmic Rejection Tools}
\label{sec:crumbs}

Cosmic Rejection Using a Multi-system Boosted decision tree Score (CRUMBS) \cite{crumbs} aims to exploit all three
 detection subsystems of SBND to reject cosmic.                                                         
Similar to the track-shower separation BDT, CRUMBS is a binary classification machine learning tool that outputs a score whether a reconstructed slice is cosmic-like or neutrino-like. 
CRUMBS takes reconstruction variables from all three detection subsystems that are complementary to each other, and therefore vastly reduces inefficiencies compared to using a single system.
The TPC information includes reconstructed variables treating a particle as both neutrino-like and cosmic-like, accounting for its topology distribution, geometrical location within the detector and calorimetry.                                                                                  
The PDS information is from the flash best matched to the slice of interest, particularly the number of PEs in the flash and the $\chi^{2}$  from the matching agreement.                        
Finally, the CRT information consists of both TPC-CRT matching algorithms, accounting for the timing for the CRT space points/tracks and the matching score.
CRUMBS was trained using the TMVA toolkit \cite{tmva} on MC samples of neutrinos, evenly distributed bet
ween $CC\nu_{\mu}$ and $CC\nu_{e}$, and cosmic muons.
The score distribution of CRUMBS shows a significant separating power between neutrino-like signals and cosmic-like backgrounds.

\subsection{Particle Identification Tools}
\label{sec:razzled}

The main tool for particle identification in the HNL search is called Razzled \cite{razzled}, which is a multi-classification BDT designed to identify particle type that would deposit energy inside SBND: $e$, $\gamma$, $\mu$, $\pi$ and $p$.
Razzled was also implemented using the TMVA toolkit \cite{tmva}.
The reconstruction variables input into the Razzled are only from using TPC reconstruction variables, mainly from the Pandora package.
There are three categories of reconstruction variables input for training Razzled: (1) generic reconstruction variables, (2) track-like variables and (3) shower-like variables.
The generic variables describe the particle multiplicity, topology, directionality and charge distribution.
Track reconstruction variables include track lengths, calorimetry, kinematics in the stopping region to identify Bragg peak, and Multiple Coulomb Scattering for $\mu$-$\pi$ separation.
Shower reconstruction variables include shower conversion gaps, opening angles and calorimetry aiming towards $\gamma$-$e$ separation.
A full description of the input variables and training can be found in Ref. \cite{EdPhD}.
By combining a collection of variables, Razzled exploits correlation between variables, significantly improving the identification performance over traditional hand cuts.
For each reconstructed particle, Razzled BDT outputs a score for each particle type and assigns the highest particle type score to that particle.

\section{Concluding Remarks}
\label{sec:reco_concluding_remarks}

The overall reconstruction workflow of SBND has been described, outlining the reconstruction process for each detection system, the TPC, the PDS and the CRT.
The three detection subsystems together provide complementary information regarding the underlying reconstructed interaction and have been used collectively by different analysis tools for different purposes.
Particularly for the HNL search, the background rejection and signal selection will be based on a combination of variables and tools employing all three subsystems to achieve a high signal-to-noise ratio, as will be discussed in the forth coming Chapter \ref{ChapterSelect}.
